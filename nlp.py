# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBAF23vG-BxNbyl1U0wiIE05RGdJcTTw

#Natural Language Processing (NLP) Assignment

###1. Text Preprocessing:
- What is tokenization in NLP? Explain its importance.


###Meaning:
* Breaking down large piece of text into smaller units  (called tokens), like words or sentences.
* As computers don't understand sentences like humans do, so breaking them into smaller parts helps the machine process the information.

###Importance
* Foundation for NLP: It's the first step in most NLP tasks and essential for analysis.
* Efficient Processing: Makes text easier for machines to handle by breaking it into smaller parts.
* Simplifies Text: Reduces complexity, focusing on important information.
* Contextual Understanding: Helps the machine to understand the meaning of words by distinguishing between similar words used in different contexts.

###Word Tokenization :
The text is split into individual words and punctuation such as comma and full-stop:

Eg: "Data science is an interdisciplinary field that uses scientific methods,
processes, algorithms, and systems to extract knowledge from data."
"""

!pip install nltk #installs the NLTK library, nltk is the name of the package to be installed.

import nltk
nltk.download('punkt') # Imports the NLTK library and downloads the 'punkt' resource for tokenization.

from nltk.tokenize import word_tokenize #Imports the word_tokenize function from NLTK, which is used to split text into individual words.

text = "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge from data." #This is my input

word_tokens = word_tokenize(text) #uses the word_tokenize function to split the text into individual words and stores the result in a variable named word_tokens.

word_tokens

"""###Sentence Tokenization
The text is treated as a single sentence because there is only one sentence and no full-stop in the sentence:
"""

from nltk.tokenize import sent_tokenize #Imports the sent_tokenize function from the NLTK library, which you can use to split text into sentences.

text = "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge from data." #This is my input

# Performing sentence tokenization
sentence_tokens = sent_tokenize(text) #The sent_tokenize function to split the text into sentences and stores the result in a variable named sentence_tokens.

sentence_tokens

"""#Stopwords in NLP. Why is it important to remove them?

Stopwords are common words like "is," "the," and "and" that don’t add much meaning in NLP tasks.

Why remove them?

* Focus on important words: It helps emphasize meaningful words.
* Reduces data size: Makes processing faster and more efficient.
* Improves accuracy: Prevents models from getting confused by irrelevant words.


 Removing stopwords simplifies text analysis without losing key information.

 Performing stopword removal on the tokenized words from the text above
"""

nltk.download('stopwords')
#This line downloads the list of stopwords from NLTK. Stopwords are common words that are often removed during text preprocessing.

from nltk.corpus import stopwords #This line imports a collection of corpus reader classes from NLTK, which can be used to access various linguistic resources like stopwords.
# Getting the list of English stopwords
stop_words = set(stopwords.words('english')) #This line creates a set of English stopwords. Stopwords are common words that are often removed during text preprocessing in NLP.

# Performing stopword removal (keeping only alphabetic words)
filtered_words = [word for word in word_tokens if word.lower() not in stop_words and word.isalpha()]
#This line of code creates a new list named filtered_words that contains all the words from word_tokens which are not stop words and contain only alphabetical characters.

filtered_words

"""##Stemming in NLP

What it is: Stemming is the process of reducing words to their base or root form by removing prefixes or suffixes. This root form doesn't have to be a real word or have a dictionary meaning, it's more about chopping off parts of the word to get to a simpler version.
"""

from nltk.stem import PorterStemmer #This line imports the PorterStemmer class from NLTK, which is a stemming algorithm used in NLP.

# Initializing the PorterStemmer
stemmer = PorterStemmer() #This line of code creates an instance of the PorterStemmer class and assigns it to the variable stemmer.

text = "improving", "processed", "arguing","analysis" #My input

# Apply stemming to each word
stemmed_words = [stemmer.stem(word) for word in text]
#This line of code creates a list named stemmed_words.
#It iterates over each word in the text variable and applies stemming to each word using the stemmer object, adding the stemmed words to this new list.

print("Original words:", text)
print("Stemmed words:", stemmed_words)

"""#2. Part-of-Speech (POS) Tagging:

###What is part-of-speech (POS) tagging? How does it help in understanding text?

POS tagging is the process of identifying and labeling each word in a sentence with its corresponding part of speech, like whether it’s a noun, verb, adjective, etc.

* Understanding context: By knowing whether a word is a noun, verb, or adjective, we get a clearer idea of its role in the sentence.

* Improves NLP accuracy: POS tagging helps models correctly interpret sentences.

POS tagging helps machines understand the function of each word in a sentence. It improves the accuracy of NLP tasks by providing context about how words are used.


Perform POS tagging on the sentence:
"Machine learning models require large datasets for accurate predictions."
"""

import nltk

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Defining the sentence
sentence = "Machine learning models require large datasets for accurate predictions."

# Tokenize the sentence into words
words = nltk.word_tokenize(sentence)

# Apply POS tagging
pos_tags = nltk.pos_tag(words)

pos_tags

nltk.download('tagsets')
# Get the full form (description) of each POS tag
print("\nFull form of POS tags:")
for word, tag in pos_tags:
    nltk.help.upenn_tagset(tag)

"""###Explain the significance of POS tags like `NN`, `VB`, and `JJ` in text analysis.

POS tags like NN (nouns), VB (verbs), and JJ (adjectives) tell us the role each word plays in a sentence. Here's why they matter:

* Word Roles: Tags like NN for nouns (people, things) and VB for verbs (actions) help us understand the structure of a sentence.

* Analyzing Sentences: POS tags show how words relate to each other, like how adjectives (JJ) describe nouns or verbs represent actions. This helps in tasks like summarizing or answering questions.

* Better Text Classification: In tasks like sentiment analysis, tags help focus on key words—like adjectives (JJ) that carry sentiment (e.g., "good" or "bad").

* Resolving Word Ambiguity: Words can have multiple meanings. POS tags tell us if a word like "book" is used as a noun (something to read) or a verb (to reserve).

* Foundation for Advanced NLP: POS tagging is a stepping stone for more complex tasks like recognizing names in a text or translating languages.

#3. Named Entity Recognition (NER):

###Named Entity Recognition (NER) is an NLP technique that automatically identifies and classifies specific entities (like names, places, or dates) in text. It's like teaching a computer to recognize the "who," "what," and "where" in a sentence.

###Common Types of Entities Identified by NER:
* Person (PER): Names of people, such as "Elon Musk" or "Marie Curie."
* Location (LOC): Places like cities, countries, or landmarks (e.g., "India," "Mount Everest").
* Organization (ORG): Companies, institutions, or groups (e.g., "Google," "United Nations").
* Date/Time (DATE/TIME): Specific dates or times (e.g., "March 2024," "10:30 AM").
* Monetary Values (MONEY): Currency amounts (e.g., "$100," "€50").
* Percentages (PERCENT): Percent values (e.g., "50%," "20 percent").
* GPE (Geo-Political Entity): Refers to geopolitical entities like countries or cities with governing structures (e.g., "France," "New York").

Perform NER on the following sentence:


"Google is planning to open a new office in New York next year."
"""

!pip install spacy

import spacy

# Load the spaCy model for English
nlp = spacy.load("en_core_web_sm")

text = "Google is planning to open a new office in New York next year."

# Apply the spaCy NLP model to the text
doc = nlp(text)

# Extract and print named entities
for ent in doc.ents:
    print(ent.text, ent.label_)

"""###Identify the entities and explain their labels

 1) Google (ORG):

* Entity: "Google"
* Label: ORG (Organization)
* Explanation: "Google" is recognized as an organization, which refers to companies or institutions.

2) New York (GPE):

* Entity: "New York"
* Label: GPE (Geo-Political Entity)
* Explanation: "New York" is recognized as a location, specifically a * geopolitical entity (city or region).

3) next year (DATE):

* Entity: "next year"
* Label: DATE
* Explanation: This refers to a time period, indicating when the event is expected to happen (opening the office "next year").

#4. Sentiment Analysis:
###What is sentiment analysis? How is it useful in understanding text data?

###Sentiment Analysis is a technique in Natural Language Processing (NLP) that determines the emotional tone behind a piece of text. It classifies text as positive, negative, or neutral based on the words used. For example:

* Positive: "I love this product!"
* Negative: "This is the worst experience."
* Neutral: "The meeting was held yesterday."

How is it Useful?

1) Customer Feedback: Companies analyze reviews to understand what customers love or dislike, helping them improve their products.

2) Brand Monitoring: Businesses track social media sentiment to gauge public opinion and respond quickly to any issues.

3) Market Research: Analyzing news and social media helps businesses spot trends and understand consumer preferences.

4) Political Insights: Sentiment analysis can assess public opinion on political matters, aiding campaign strategies.

5) Personalized Content: By understanding user sentiments, companies can recommend content that aligns with what users enjoy.

6) Better Customer Support: Recognizing negative sentiments allows businesses to prioritize responses and improve customer interactions.

sentiment analysis helps us understand emotions in text, guiding businesses to make better decisions and connect more effectively with their audience.

###Perform sentiment analysis on the following text:
* "The project outcome was highly satisfying, and the team did an excellent job."
* "The service was terrible, and the support was unresponsive."
"""

!pip install textblob

from textblob import TextBlob

texts = [
    "The project outcome was highly satisfying, and the team did an excellent job.",
    "The service was terrible, and the support was unresponsive."
    ]

# Perform sentiment analysis
for text in texts:
    blob = TextBlob(text)
    sentiment = blob.sentiment.polarity
    sentiment_label = "Positive" if sentiment > 0 else "Negative" if sentiment < 0 else "Neutral"
    print(f"Text: '{text}'")
    print(f"Sentiment Score: {sentiment}, Sentiment: {sentiment_label}\n")

"""###Explain the sentiment results and their interpretation.

TextBlob Sentiment Analysis:

The TextBlob object allows you to easily analyze the sentiment of the text. The sentiment.polarity method returns a float between -1.0 and 1.0, where negative values indicate negative sentiment and positive values indicate positive sentiment.


For Text: 'The project outcome was highly satisfying, and the team did an excellent job.'

* Sentiment Score: 0.75
* Interpretation: The polarity score is positive, suggesting a positive sentiment. A score of 0.75 indicates a relatively strong positive feeling regarding the project outcome and the team’s effort.


For Text: 'The service was terrible, and the support was unresponsive.'


* Sentiment Score:  -1.0
* Interpretation: The polarity score is negative, signifying a negative sentiment. A score of  -1.0 suggests a strong negative feeling regarding the service and support.

#5. Text Generation:

###What is text generation in NLP? Provide real-world applications of text generation models.

####Text Generation in Natural Language Processing (NLP) is the ability of computers to produce human-like text based on input or context. Models like GPT learn from vast amounts of text data to create coherent and relevant sentences or paragraphs.

##Real-World Applications of Text Generation

1) Content Creation:

* Automated tools can generate blog posts, articles, and social media content, saving time for writers.

2) Chatbots:

* Text generation enables chatbots to provide human-like responses in customer service, enhancing user experience.

3) Creative Writing:

* Authors use text generation to brainstorm ideas, plot outlines, or even generate entire stories, helping overcome writer's block.

4) Personalized Marketing:

* Companies generate tailored emails and advertisements based on user data for more effective marketing.

5) Educational Tools:

* Text generation models create quizzes, summaries, and explanations, providing additional resources for students and educators.

6) Translation Services:

* Automated translation generates contextually accurate translations, facilitating communication across languages.

7) Game Development:

* Dynamic dialogues and narratives in video games can be generated, enhancing the gaming experience.


 Text generation is like having a smart assistant that writes for you. It saves time and boosts creativity across various fields, from content creation to customer service.

###Use a pre-trained language model to generate text for the prompt:
"Artificial intelligence is revolutionizing
"""

# Importing the necessary library
from transformers import pipeline

# Initialize the text generation pipeline
generator = pipeline("text-generation")

# Generate text based on the provided prompt
generated_text = generator("Artificial intelligence is revolutionizing", max_length=200)

# Print the generated text
print("Generated Text:", generated_text[0]['generated_text'])

"""###Discuss how text generation models like GPT work.

###How Text Generation Models Like GPT Work

####Text generation models like GPT (Generative Pre-trained Transformer) create coherent and contextually relevant text based on prompts.

1) Transformer Architecture:

GPT is built on a framework called the Transformer, which uses a self-attention mechanism to understand the relationships between words in a sentence. This helps the model determine which words are important for generating the next word.

2) Pre-training:

The model learns from a massive amount of text data like books, articles, and websites. During this phase, it predicts the next word in a sentence, helping it grasp grammar, facts, and some world knowledge without any labeled data.

3) Fine-tuning:

After pre-training, GPT can be adjusted for specific tasks, like summarizing text or answering questions, using smaller, targeted datasets.

4) Text Generation Process:

* Input Prompt: You give the model a starting sentence or phrase (like "Artificial intelligence is revolutionizing").
* Tokenization: The input is broken down into smaller pieces (tokens) that the model can understand.

* Self-Attention: The model analyzes these tokens to see how they relate to one another, which guides its predictions.
* Generating Text: It generates one word at a time, adding each new word to the input and repeating the process until it reaches the desired length.

5) Decoding Strategies:

The model can use different methods to generate text:
Greedy Search: Chooses the most likely next word at each step but may lead to repetitive text.
Sampling: Adds randomness, making the output more varied and interesting.

6) Real-World Applications:

Creative Writing: Helps authors generate stories or dialogues.
Chatbots: Powers customer service bots that respond naturally to users.
Content Creation: Automates writing for articles and social media posts.
Language Translation: Assists in translating text between languages.

#6. Text Summarization:

- What is text summarization? Differentiate between extractive and abstractive
summarization techniques.

Text summarization is the process of condensing a large amount of text into a shorter version while retaining the main ideas and key information. It's like reading a long article and being able to explain it in just a few sentences.


###Extractive Summarization:

* How It Works: This technique involves selecting and pulling out sentences or phrases directly from the original text. It’s like picking the most important quotes from a book to create a summary.
* Pros: The summary is guaranteed to be grammatically correct since it uses the original text.
* Cons: It may miss the overall context or flow, resulting in a choppy summary.

###Abstractive Summarization:

* How It Works: This approach generates new sentences that capture the essence of the original text, rather than just copying it. It’s similar to paraphrasing—rewriting the content in your own words.
* Pros: The summary can provide a more coherent and contextually relevant overview of the text.
* Cons: It requires more advanced algorithms and may sometimes generate inaccurate information.

###Summarize the following text into 2-3 sentences:
"Artificial intelligence refers to the simulation of human intelligence in machines
that are programmed to think like humans and mimic their actions. AI is being
applied in a wide range of industries, from healthcare to finance, and has the
potential to improve efficiency and decision-making."
"""

from transformers import pipeline

# Initialize the text summarization pipeline
summarizer = pipeline("summarization")

# Input text to summarize
text = ("Artificial intelligence refers to the simulation of human intelligence in machines "
        "that are programmed to think like humans and mimic their actions. AI is being "
        "applied in a wide range of industries, from healthcare to finance, and has the "
        "potential to improve efficiency and decision-making.")

# Generate summary
summary = summarizer(text, max_length=50, min_length=25)
print("Summary:", summary[0]['summary_text'])

"""###Importance of Summarization in NLP
* Text summarization is crucial in Natural Language Processing (NLP) for several reasons:

* Efficiency: With the overwhelming amount of information available today, summarization helps individuals quickly grasp the main points without reading lengthy documents. It saves time and allows for faster decision-making.

* Information Overload: In a world filled with data, summarization acts as a filter, distilling relevant information from noise. This is especially valuable for professionals who need to stay updated on industry news or research without being overwhelmed.

* Improved Comprehension: Summarizing complex texts into simpler, more digestible formats enhances understanding, making it easier for readers to grasp essential concepts and ideas.

* Content Management: Businesses can use summarization to create concise reports, executive summaries, and even social media posts, making it easier to communicate key messages.

### Report on Insights Gained from NLP Tasks
Natural Language Processing (NLP) helps in several techniques that facilitate the understanding and processing of human language. Here’s a summary of insights gained from key NLP tasks:

1) Tokenization:

Insight: Tokenization breaks text into smaller units, such as words or sentences. This helps in analyzing the structure and meaning of the text, enabling subsequent tasks to focus on individual components, making it easier to understand and manipulate the content.

2) Stopword Removal:

Insight: Removing stopwords (common words like "and," "the," "is") eliminates noise from the text. This enhances the focus on meaningful words, improving the efficiency of text analysis and allowing models to concentrate on significant terms that contribute to the overall meaning.

3) Part-of-Speech (POS) Tagging:

Insight: POS tagging assigns labels (like noun, verb, adjective) to words in a sentence. This helps in understanding the grammatical structure, enabling models to grasp relationships between words and enhancing comprehension of the text's meaning.

4) Named Entity Recognition (NER):

Insight: NER identifies and classifies key entities (like names, locations, organizations) within text. This provides valuable contextual information and helps in understanding the relevance and importance of different elements in the content.

5) Sentiment Analysis:

Insight: Sentiment analysis determines the emotional tone behind text (positive, negative, neutral). This is crucial for understanding opinions, attitudes, and feelings expressed in the content, making it valuable for applications like social media monitoring and customer feedback analysis.

6) Text Generation:

Insight: Text generation models create coherent and contextually relevant text based on prompts. This ability to produce human-like text facilitates creative writing, automated content creation, and personalized communication.

7) Summarization:

Insight: Summarization condenses lengthy texts into concise summaries, allowing for quick understanding of key points. This is essential in managing information overload and ensuring that important insights are easily accessible.

Each of these NLP tasks contributes to a more clear understanding of language by breaking down text into manageable parts, focusing on meaningful content, and providing valuable insights. Together, they enhance our ability to process, analyze, and generate human language effectively, making NLP an essential tool in the digital age.
"""